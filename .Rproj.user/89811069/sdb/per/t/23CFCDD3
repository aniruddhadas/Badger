{
    "contents" : "---\ntitle       : Random forests\nsubtitle    : \nauthor      : Jeffrey Leek, Assistant Professor of Biostatistics \njob         : Johns Hopkins Bloomberg School of Public Health\nlogo        : bloomberg_shield.png\nframework   : io2012        # {io2012, html5slides, shower, dzslides, ...}\nhighlighter : highlight.js  # {highlight.js, prettify, highlight}\nhitheme     : tomorrow   # \nurl:\n  lib: ../../libraries\n  assets: ../../assets\nwidgets     : [mathjax]            # {mathjax, quiz, bootstrap}\nmode        : selfcontained # {standalone, draft}\n---\n\n\n\n\n\n\n## Random forests\n\n1. Bootstrap samples\n2. At each split, bootstrap variables\n3. Grow multiple trees and vote\n\n__Pros__:\n\n1. Accuracy\n\n__Cons__:\n\n1. Speed\n2. Interpretability\n3. Overfitting\n\n\n---\n\n## Random forests\n\n<img class=center src=../../assets/img/08_PredictionAndMachineLearning/forests.png height=400>\n\n[http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf](http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf)\n\n\n---\n\n## Iris data\n\n\n```r\ndata(iris); library(ggplot2)\ninTrain <- createDataPartition(y=iris$Species,\n                              p=0.7, list=FALSE)\ntraining <- iris[inTrain,]\ntesting <- iris[-inTrain,]\n```\n\n\n\n---\n\n## Random forests\n\n\n```r\nlibrary(caret)\nmodFit <- train(Species~ .,data=training,method=\"rf\",prox=TRUE)\nmodFit\n```\n\n```\n105 samples\n  4 predictors\n  3 classes: 'setosa', 'versicolor', 'virginica' \n\nNo pre-processing\nResampling: Bootstrap (25 reps) \n\nSummary of sample sizes: 105, 105, 105, 105, 105, 105, ... \n\nResampling results across tuning parameters:\n\n  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD\n  2     0.9       0.9    0.03         0.04    \n  3     0.9       0.9    0.03         0.05    \n  4     0.9       0.9    0.03         0.05    \n\nAccuracy was used to select the optimal model using  the largest value.\nThe final value used for the model was mtry = 3. \n```\n\n\n---\n\n## Getting a single tree\n\n\n```r\ngetTree(modFit$finalModel,k=2)\n```\n\n```\n   left daughter right daughter split var split point status prediction\n1              2              3         4        0.70      1          0\n2              0              0         0        0.00     -1          1\n3              4              5         4        1.70      1          0\n4              6              7         3        4.95      1          0\n5              8              9         3        4.85      1          0\n6              0              0         0        0.00     -1          2\n7             10             11         4        1.55      1          0\n8             12             13         1        5.95      1          0\n9              0              0         0        0.00     -1          3\n10             0              0         0        0.00     -1          3\n11             0              0         0        0.00     -1          2\n12             0              0         0        0.00     -1          2\n13             0              0         0        0.00     -1          3\n```\n\n\n---\n\n## Class \"centers\"\n\n\n```r\nirisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)\nirisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)\np <- qplot(Petal.Width, Petal.Length, col=Species,data=training)\np + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)\n```\n\n<div class=\"rimage center\"><img src=\"fig/centers.png\" title=\"plot of chunk centers\" alt=\"plot of chunk centers\" class=\"plot\" /></div>\n\n\n---\n\n## Predicting new values\n\n\n```r\npred <- predict(modFit,testing); testing$predRight <- pred==testing$Species\ntable(pred,testing$Species)\n```\n\n```\n            \npred         setosa versicolor virginica\n  setosa         15          0         0\n  versicolor      0         14         1\n  virginica       0          1        14\n```\n\n\n---\n\n## Predicting new values\n\n\n```r\nqplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main=\"newdata Predictions\")\n```\n\n<div class=\"rimage center\"><img src=\"fig/unnamed-chunk-2.png\" title=\"plot of chunk unnamed-chunk-2\" alt=\"plot of chunk unnamed-chunk-2\" class=\"plot\" /></div>\n\n\n---\n\n## Notes and further resources\n\n__Notes__:\n\n* Random forests are usually one of the two top\nperforming algorithms along with boosting in prediction contests.\n* Random forests are difficult to interpret but often very accurate. \n* Care should be taken to avoid overfitting (see [rfcv](http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) funtion)\n\n\n__Further resources__:\n\n* [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)\n* [Random forest Wikipedia](http://en.wikipedia.org/wiki/Random_forest)\n* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)\n",
    "created" : 1411190749240.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3263806842",
    "id" : "23CFCDD3",
    "lastKnownWriteTime" : 1410050908,
    "path" : "~/GitHub/courses/08_PracticalMachineLearning/021randomForests/index.md",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "markdown"
}