{
    "contents" : "---\ntitle       : Preprocessing with Principal Components Analysis (PCA)\nsubtitle    : \nauthor      : Jeffrey Leek\njob         : Johns Hopkins Bloomberg School of Public Health\nlogo        : bloomberg_shield.png\nframework   : io2012        # {io2012, html5slides, shower, dzslides, ...}\nhighlighter : highlight.js  # {highlight.js, prettify, highlight}\nhitheme     : tomorrow   # \nurl:\n  lib: ../../libraries\n  assets: ../../assets\nwidgets     : [mathjax]            # {mathjax, quiz, bootstrap}\nmode        : selfcontained # {standalone, draft}\n---\n\n\n```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}\n# make this an external chunk that can be included in any file\noptions(width = 100)\nopts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')\n\noptions(xtable.type = 'html')\nknit_hooks$set(inline = function(x) {\n  if(is.numeric(x)) {\n    round(x, getOption('digits'))\n  } else {\n    paste(as.character(x), collapse = ', ')\n  }\n})\nknit_hooks$set(plot = knitr:::hook_plot_html)\n```\n\n\n## Correlated predictors\n\n```{r loadPackage,cache=TRUE,fig.height=3.5,fig.width=3.5}\nlibrary(caret); library(kernlab); data(spam)\ninTrain <- createDataPartition(y=spam$type,\n                              p=0.75, list=FALSE)\ntraining <- spam[inTrain,]\ntesting <- spam[-inTrain,]\n\nM <- abs(cor(training[,-58]))\ndiag(M) <- 0\nwhich(M > 0.8,arr.ind=T)\n```\n\n---\n\n## Correlated predictors\n\n```{r,dependson=\"loadPackage\",cache=TRUE,fig.height=3.5,fig.width=3.5}\nnames(spam)[c(34,32)]\nplot(spam[,34],spam[,32])\n```\n\n\n---\n\n## Basic PCA idea\n\n* We might not need every predictor\n* A weighted combination of predictors might be better\n* We should pick this combination to capture the \"most information\" possible\n* Benefits\n  * Reduced number of predictors\n  * Reduced noise (due to averaging)\n\n\n---\n\n## We could rotate the plot\n\n$$ X = 0.71 \\times {\\rm num 415} + 0.71 \\times {\\rm num857}$$\n\n$$ Y = 0.71 \\times {\\rm num 415} - 0.71 \\times {\\rm num857}$$\n\n```{r,dependson=\"loadPackage\",cache=TRUE,fig.height=3.5,fig.width=3.5}\nX <- 0.71*training$num415 + 0.71*training$num857\nY <- 0.71*training$num415 - 0.71*training$num857\nplot(X,Y)\n```\n\n---\n\n## Related problems\n\nYou have multivariate variables $X_1,\\ldots,X_n$ so $X_1 = (X_{11},\\ldots,X_{1m})$\n\n* Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible.\n* If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data.\n\n\nThe first goal is <font color=\"#330066\">statistical</font> and the second goal is <font color=\"#993300\">data compression</font>.\n\n---\n\n## Related solutions - PCA/SVD\n\n__SVD__\n\nIf $X$ is a matrix with each variable in a column and each observation in a row then the SVD is a \"matrix decomposition\"\n\n$$ X = UDV^T$$\n\nwhere the columns of $U$ are orthogonal (left singular vectors), the columns of $V$ are orthogonal (right singluar vectors) and $D$ is a diagonal matrix (singular values). \n\n__PCA__\n\nThe principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.\n\n---\n\n## Principal components in R - prcomp\n\n```{r prcomp,dependson=\"loadPackage\",cache=TRUE,fig.height=3.5,fig.width=3.5}\nsmallSpam <- spam[,c(34,32)]\nprComp <- prcomp(smallSpam)\nplot(prComp$x[,1],prComp$x[,2])\n```\n\n---\n\n## Principal components in R - prcomp\n\n```{r ,dependson=\"prcomp\",cache=TRUE,fig.height=3.5,fig.width=3.5}\nprComp$rotation\n```\n\n\n---\n\n## PCA on SPAM data\n\n```{r spamPC,dependson=\"loadPackage\",cache=TRUE,fig.height=3.5,fig.width=3.5}\ntypeColor <- ((spam$type==\"spam\")*1 + 1)\nprComp <- prcomp(log10(spam[,-58]+1))\nplot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab=\"PC1\",ylab=\"PC2\")\n```\n\n\n---\n\n## PCA with caret\n\n```{r ,dependson=\"spamPC\",cache=TRUE,fig.height=3.5,fig.width=3.5}\npreProc <- preProcess(log10(spam[,-58]+1),method=\"pca\",pcaComp=2)\nspamPC <- predict(preProc,log10(spam[,-58]+1))\nplot(spamPC[,1],spamPC[,2],col=typeColor)\n```\n\n\n---\n\n## Preprocessing with PCA\n\n```{r pcaCaret,dependson=\"spamPC\",cache=TRUE,fig.height=3.5,fig.width=3.5}\npreProc <- preProcess(log10(training[,-58]+1),method=\"pca\",pcaComp=2)\ntrainPC <- predict(preProc,log10(training[,-58]+1))\nmodelFit <- train(training$type ~ .,method=\"glm\",data=trainPC)\n```\n\n---\n\n## Preprocessing with PCA\n\n```{r ,dependson=\"pcaCaret\",cache=TRUE,fig.height=3.5,fig.width=3.5}\ntestPC <- predict(preProc,log10(testing[,-58]+1))\nconfusionMatrix(testing$type,predict(modelFit,testPC))\n```\n\n---\n\n## Alternative (sets # of PCs)\n\n```{r ,dependson=\"pcaCaret\",cache=TRUE,fig.height=3.5,fig.width=3.5}\nmodelFit <- train(training$type ~ .,method=\"glm\",preProcess=\"pca\",data=training)\nconfusionMatrix(testing$type,predict(modelFit,testing))\n```\n\n---\n\n## Final thoughts on PCs\n\n* Most useful for linear-type models\n* Can make it harder to interpret predictors\n* Watch out for outliers! \n  * Transform first (with logs/Box Cox)\n  * Plot predictors to identify problems\n* For more info see \n  * Exploratory Data Analysis\n  * [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n  ",
    "created" : 1411191397173.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2313956541",
    "id" : "EDCFCF0",
    "lastKnownWriteTime" : 1410050908,
    "path" : "~/GitHub/courses/08_PracticalMachineLearning/016preProcessingPCA/index.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}